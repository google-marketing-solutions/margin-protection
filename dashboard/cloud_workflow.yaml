# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Loads data from a Drive ID into BigQuery for Performance Monitor.
#
# Provided with a dataset ID, a drive ID and a cloud function URL for BigQuery loading, this
# workflow will:
#
# 1. Get a list of files from Google Drive incrementally from after the last data load.
# 2. Pass those files into the Cloud Function responsible for loading data into Cloud Function.
#
# This workflow can be built on: Add new views, delete old data, or whatever you like! This is a
# base that makes logic simple to add without code.
main:
  params: [args]
  steps:
    - init:
        assign:
          - project_id: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - dataset_id: "performance_monitor"
          - drive_id: ${args.drive_id}
          - cloud_function_url: ${args.cloud_function_url}
          - after: ""
    - create_dataset:
        call: create_dataset
        args:
          dataset_id: ${dataset_id}
          project_id: ${project_id}
    - get_last_report_date:
        call: get_last_report_date
        args:
          - ${project_id}
          - ${dataset_id}
        result: date
    - define_after_var:
        switch:
          - condition: ${date == ""}
            assign:
              - after: ""
          - condition: ${date != ""}
            assign:
              - after: ${" and createdTime >= \"" + date + "\""}
    - load_files_into_bigquery:
        call: load_files_into_bigquery
        args:
          drive_id: ${drive_id}
          project_id: ${project_id}
          dataset_id: ${dataset_id}
          cloud_function_url: ${cloud_function_url}
          after: ${after}
        result: tables
    - done:
        return: ${tables}

# Returns the last report date from the table.
# If there is no table then it returns a blank.
get_last_report_date:
  params: [project_id, dataset_id]
  steps:
    - init:
        assign:
          - project_id: ${project_id}
          - dataset_id: ${dataset_id}
          - items: { rows: [f: [{ v: "" }]] }
    - get_last_report_date:
        try:
          call: googleapis.bigquery.v2.jobs.query
          args:
            projectId: ${project_id}
            body:
              kind: "query"
              defaultDataset:
                datasetId: ${dataset_id}
                projectId: ${project_id}
              query: 'SELECT FORMAT_TIMESTAMP("%Y-%m-%dT%H:%M:%SZ", Date) AS date FROM last_report ORDER BY Date DESC LIMIT 1'
              useLegacySql: false
          result: items
        except:
          as: e
          steps:
            - known_errors:
                switch:
                  - condition: "${e.code == 404}"
                    return: ""
            - unhandled_exception:
                raise: ${e}
    - output:
        call: sys.log
        args:
          text: ${items}
          severity: INFO
    - route_done:
        switch:
          - condition: '${map.get(items, "rows") == null}'
            next: done_sad
    - done_happy:
        return: ${items.rows[0].f[0].v}
    - done_sad:
        return: ""

retry_logic:
  params: [e]
  steps:
    - should_repeat:
        switch:
          - condition: ${"LookupError" in e.tags}
            return: true
    - fail:
        return: false

load_files_into_bigquery:
  params:
    [
      drive_id,
      project_id,
      dataset_id,
      cloud_function_url,
      after,
      page_token: "",
    ]
  steps:
    - init:
        assign:
          - q: ${"'" + drive_id + "' in parents and trashed=false"}
          - table_map: {}
    - get_files:
        switch:
          - condition: '${page_token != ""}'
            call: drive_query
            args:
              orderBy: createdTime
              pageSize: 500
              q: '${q + " " + after}'
            result: files
          - condition: '${page_token == ""}'
            call: drive_query
            args:
              orderBy: createdTime
              pageSize: 200
              q: '${q + " " + after}'
              pageToken: "${page_token}"
            result: files
    - add_to_bigquery:
        call: add_files_to_bigquery
        args:
          files: ${files.body.files}
          project_id: ${project_id}
          dataset_id: ${dataset_id}
          cloud_function_url: ${cloud_function_url}
        result: tables
    - assign_tables_to_map:
        for:
          value: table
          in: ${keys(tables)}
          steps:
            - add_to_map_2:
                assign:
                  - table_map[table]: ""
    - clear_memory:
        assign:
          - files:
    - next_page:
        switch:
          - condition: "${files == null}"
            next: done
          - condition: '${files.body.nextPageToken != ""}'
            call: load_files_into_bigquery
            args:
              drive_id: ${drive_id}
              page_token: ${files.body.nextPageToken}
              project_id: ${project_id}
              dataset_id: ${dataset_id}
              cloud_function_url: ${cloud_function_url}
              after: ${after}
            result: new_table_map
          - condition: '${files.body.nextPageToken == ""}'
            next: add_to_bigquery
    - merge_table_map:
        for:
          value: table_key
          in: ${keys(new_table_map)}
          steps:
            - add_to_map:
                assign:
                  - table_map[table_key]: ""
    - done:
        return: ${table_map}

drive_query:
  params: [q, orderBy, pageSize, pageToken: null]
  steps:
    - init:
        assign:
          - query:
              q: ${q}
              orderBy: ${orderBy}
              pageSize: ${pageSize}
    - add_page_token:
        switch:
          - condition: "${pageToken != null}"
            assign:
              - query.pageToken: ${pageToken}
    - make_request:
        call: http.get
        args:
          url: https://www.googleapis.com/drive/v3/files
          query: ${query}
          auth:
            type: OAuth2
            scopes:
              - https://www.googleapis.com/auth/drive
        result: files
    - done:
        return: ${files}

add_files_to_bigquery:
  params: [files, project_id, dataset_id, cloud_function_url]
  steps:
    - create_insert_job:
        call: http.post
        args:
          url: ${cloud_function_url}
          body:
            gcp_project: ${project_id}
            gcp_dataset: ${dataset_id}
            file_list: ${files}
          auth:
            type: OIDC
        result: tables
    - done:
        return: ${tables}

create_dataset:
  params: [project_id, dataset_id]
  steps:
    - create:
        try:
          call: googleapis.bigquery.v2.datasets.insert
          args:
            projectId: ${project_id}
            body:
              datasetReference:
                projectId: ${project_id}
                datasetId: ${dataset_id}
              access[].role: "roles/bigquery.dataViewer"
        except:
          as: e
          steps:
            - known_errors:
                switch:
                  - condition: '${"HttpError" in e.tags}'
                    return: "Already created"
            - fail:
                raise: ${e}
